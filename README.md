# Leveraging ParsBERT and Pretrained mT5 for Persian Abstractive Text Summarization


Paper presenting: arXiv:2012.11204
Paper link: https://arxiv.org/pdf/2012.11204.pdf


## Introduction
Natural Language Processing (NLP) is a field of AI that focuses on processing textual information in order to make them comprehendable to computers. With the emergence of Deep Learning (DL), numerous DL-based models and architectures have been proposed for different NLP tasks such as Named Entity Recognition (NER), Sentiment Analysis (SA) and Question/Answering (QA). One of the most recent and most popular approaches towards these tasks is to use pre-trained language models. Pre-trained language models used for NLP tasks are essentially huge neural networks employing Long Short-Term Memory (LSTM) architcture that are trained on enormous text corpus. A few examples include BERT and T5 models.  BERT is an encoder-only model that uses Masked Language Model (MLM) to create joint conditioning on left and right context. T5 is a sequence-to-sequnce (seq2seq) framework that creates a text-to-text format to address NLP tasks. However, regardless of the architecture, any pre-trained model has to be fine-tuned towards any of the NLP tasks using an appropriate dataset.
There are numerous NLP datasets available for different tasks, especially for the English language. Some tasks, however, have had a lesser fortune regarding the amount of textual data available.
In all such tasks, the objective is to create a language model that could create abstract encoded representations of an input text that could later be used to 

## Dataset Information
...

## Evaluation
...

## Citation
...

## Contributors
...

## License
[MIT License](LICENSE)